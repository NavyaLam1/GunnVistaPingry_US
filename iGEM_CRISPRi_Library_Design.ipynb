{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Learning sgRNA predictors from empirical data\n",
    "    * Load scripts and empirical data\n",
    "    * Generate TSS annotation using FANTOM dataset\n",
    "    * Calculate parameters for empirical sgRNAs\n",
    "    * Fit parameters\n",
    "2. Applying machine learning model to predict sgRNA activity\n",
    "    * Find all sgRNAs in genomic regions of interest \n",
    "    * Predicting sgRNA activity\n",
    "3. Construct sgRNA libraries\n",
    "    * Score sgRNAs for off-target potential\n",
    "* Pick the top sgRNAs for a library, given predicted activity scores and off-target filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Learning sgRNA predictors from empirical data\n",
    "## Load scripts and empirical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#modify below variable to point to the base directory containing large genome data files large_data_files directory.\n",
    "#LARGE_FILE_DIR= '../../../../data/'\n",
    "LARGE_FILE_DIR= './'\n",
    "\n",
    "FASTA_FILE_OF_GENOME= LARGE_FILE_DIR + 'large_data_files/hg19.fa'\n",
    "GTF_FILE_FROM_GENCODE = LARGE_FILE_DIR + 'large_data_files/gencode.v19.annotation.gtf'\n",
    "\n",
    "TSS_TABLE_PATH='data_files/human_tssTable.txt'\n",
    "P1P2_TABLE_PATH='data_files/human_p1p2Table.txt'\n",
    "\n",
    "FANTOM_TSS_ANNOTATION_BED= LARGE_FILE_DIR + 'large_data_files/TSS_human.sorted.bed.gz'\n",
    "HGNC_SYMBOL_LOOKUP_TABLE= LARGE_FILE_DIR + 'large_data_files/hgnc_complete_set_2020-08-01.txt'\n",
    "\n",
    "#spreadsheet containing the lab experiment data to train the model\n",
    "IGEM_EXCEL_FILE= 'data_files/Sam_Perli_qRT_pcr_data_per_gene.xlsx'\n",
    "\n",
    "\n",
    "PICKLE_FILE = 'igem_v1_estimator'\n",
    "TRANSFORMED_PARAM_HEADER='igem_v1_transformed_param_header'\n",
    "\n",
    "PREDICTED_SCORE_TABLE='igem_v1_predicted_score_table'\n",
    "TEMP_FASTQ_FILE='igem_v1_temp_fastq_file'\n",
    "\n",
    "\n",
    "%run sgRNA_learning.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the experiment data\n",
    "df=pd.read_excel(open(IGEM_EXCEL_FILE, 'rb'),\n",
    "              sheet_name='Sheet1')  \n",
    "df =df.dropna( subset = ['sgID'])\n",
    "\n",
    "df_research = df.set_index('sgID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "libraryTable_research = df[['sgID','gene','transcript','protospacer sequence']]\n",
    "libraryTable_research= libraryTable_research.loc[1:,].set_index('sgID').rename(columns={'protospacer sequence':'sequence','transcript':'transcripts'})\n",
    "\n",
    "#Exclude rows relating to the following gene so that we can calculate scores once we train our model\n",
    "GENE_TO_EXCLUDE= 'EIF4G1'\n",
    "\n",
    "#Select all genes except last one\n",
    "libraryTable_subset = libraryTable_research[ libraryTable_research['gene'] != GENE_TO_EXCLUDE ]\n",
    "#Read sgID\n",
    "sgInfoTable = parseAllSgIds(libraryTable_subset)\n",
    "\n",
    "#Read scores measured in the lab\n",
    "normedScores = df[['sgID','gene','Measured by Dr. Perli']].loc[1:,]\n",
    "normedScores = normedScores [ normedScores['gene'] != GENE_TO_EXCLUDE]\n",
    "\n",
    "#normedScores['Measured by Dr. Perli']= np.log10(1/normedScores['Measured by Dr. Perli'])\n",
    "#For the CRISPRi scores measured in the Dr. Perli's lab with qRT-PCR lower the score better the guide, but in our model we will be predicting in such a way that better the score better the guide\n",
    "#Subtract the scores measured in lab from 1\n",
    "normedScores['Measured by Dr. Perli']= 1-normedScores['Measured by Dr. Perli']\n",
    "normedScores = normedScores [['sgID','Measured by Dr. Perli']].set_index('sgID').rename(columns={'Measured by Dr. Perli':''})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgInfoTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normedScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load genome data\n",
    "genomeDict = loadGenomeAsDict(FASTA_FILE_OF_GENOME)\n",
    "gencodeData = loadGencodeData(GTF_FILE_FROM_GENCODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read TSS annotation generated using FANTOM dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "tssTable = pd.read_csv(TSS_TABLE_PATH,sep='\\t', index_col=[0,1])\n",
    "p1p2Table = pd.read_csv(P1P2_TABLE_PATH,sep='\\t', header=0, index_col=[0,1], converters={\"primary TSS\": ast.literal_eval, \"secondary TSS\": ast.literal_eval})\n",
    "\n",
    "p1p2Table.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Calculate parameters for empirical sgRNAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load bigwig files for any chromatin data of interest\n",
    "bwhandleDict = {'dnase':BigWigFile(open(LARGE_FILE_DIR + 'large_data_files/wgEncodeOpenChromDnaseK562BaseOverlapSignalV2.bigWig','rb')),\n",
    "'faire':BigWigFile(open(LARGE_FILE_DIR + 'large_data_files/wgEncodeOpenChromFaireK562Sig.bigWig','rb')),\n",
    "'mnase':BigWigFile(open(LARGE_FILE_DIR + 'large_data_files/wgEncodeSydhNsomeK562Sig.bigWig','rb'))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "paramTable_trainingGuides = generateTypicalParamTableEx(libraryTable_subset,sgInfoTable, tssTable, p1p2Table, genomeDict, bwhandleDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramTable_trainingGuides.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramTable_trainingGuides.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#populate table of fitting parameters\n",
    "typeList = ['binnable_onehot', \n",
    "            'continuous', 'continuous', 'continuous', 'continuous',\n",
    "            'continuous', 'continuous', 'continuous', 'continuous',\n",
    "            'binnable_onehot','binnable_onehot','binnable_onehot','binnable_onehot',\n",
    "            'binnable_onehot','binnable_onehot','binnable_onehot','binnable_onehot','binnable_onehot','binnable_onehot','binnable_onehot',\n",
    "            'binary']\n",
    "typeList.extend(['binary']*160)\n",
    "typeList.extend(['binary']*(16*38))\n",
    "typeList.extend(['binnable_onehot']*3)\n",
    "typeList.extend(['binnable_onehot']*2)\n",
    "typeList.extend(['binary']*18)\n",
    "fitTable = pd.DataFrame(typeList, index=paramTable_trainingGuides.columns, columns=['type'])\n",
    "MIN_EDGE_DATA=10\n",
    "fitparams =[{'bin width':1, 'min edge data':MIN_EDGE_DATA, 'bin function':np.median},\n",
    "            {'C':[.01,.05, .1,.5], 'gamma':[.000001, .00005,.0001,.0005]},\n",
    "            {'C':[.01,.05, .1,.5], 'gamma':[.000001, .00005,.0001,.0005]},\n",
    "            {'C':[.01,.05, .1,.5], 'gamma':[.000001, .00005,.0001,.0005]},\n",
    "            {'C':[.01,.05, .1,.5], 'gamma':[.000001, .00005,.0001,.0005]},\n",
    "            {'C':[.01,.05, .1,.5], 'gamma':[.000001, .00005,.0001,.0005]},\n",
    "            {'C':[.01,.05, .1,.5], 'gamma':[.000001, .00005,.0001,.0005]},\n",
    "            {'C':[.01,.05, .1,.5], 'gamma':[.000001, .00005,.0001,.0005]},\n",
    "            {'C':[.01,.05, .1,.5], 'gamma':[.000001, .00005,.0001,.0005]},\n",
    "            {'bin width':1, 'min edge data':MIN_EDGE_DATA, 'bin function':np.median},\n",
    "            {'bin width':1, 'min edge data':MIN_EDGE_DATA, 'bin function':np.median},\n",
    "            {'bin width':1, 'min edge data':MIN_EDGE_DATA, 'bin function':np.median},\n",
    "            {'bin width':1, 'min edge data':MIN_EDGE_DATA, 'bin function':np.median},\n",
    "            {'bin width':.1, 'min edge data':MIN_EDGE_DATA, 'bin function':np.median},\n",
    "            {'bin width':.1, 'min edge data':MIN_EDGE_DATA, 'bin function':np.median},\n",
    "            {'bin width':.1, 'min edge data':MIN_EDGE_DATA, 'bin function':np.median},\n",
    "            {'bin width':.1, 'min edge data':MIN_EDGE_DATA, 'bin function':np.median},\n",
    "            {'bin width':.1, 'min edge data':MIN_EDGE_DATA, 'bin function':np.median},\n",
    "            {'bin width':.1, 'min edge data':MIN_EDGE_DATA, 'bin function':np.median},\n",
    "            {'bin width':.1, 'min edge data':MIN_EDGE_DATA, 'bin function':np.median},\n",
    "            dict()]\n",
    "fitparams.extend([dict()]*160)\n",
    "fitparams.extend([dict()]*(16*38))\n",
    "fitparams.extend([\n",
    "            {'bin width':.15, 'min edge data':MIN_EDGE_DATA, 'bin function':np.median},\n",
    "            {'bin width':.15, 'min edge data':MIN_EDGE_DATA, 'bin function':np.median},\n",
    "            {'bin width':.15, 'min edge data':MIN_EDGE_DATA, 'bin function':np.median}])\n",
    "fitparams.extend([\n",
    "            {'bin width':2, 'min edge data':MIN_EDGE_DATA, 'bin function':np.median},\n",
    "            {'bin width':2, 'min edge data':MIN_EDGE_DATA, 'bin function':np.median}])\n",
    "fitparams.extend([dict()]*18)\n",
    "fitTable['params'] = fitparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate random gene folds containing training and test data. We will select the best estimater that got generated out of these random sets\n",
    "geneFoldList = getGeneFoldsEx(libraryTable_subset, 17, transcripts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each fold, fit parameters to training folds and measure R^2 of test scores\n",
    "#Use Elastic Net CV linear regression\n",
    "coefs = []\n",
    "metricTups = []\n",
    "\n",
    "for geneFold_train, geneFold_test in geneFoldList:\n",
    "\n",
    "    transformedParams_train, estimators = fitParams(paramTable_trainingGuides.loc[normedScores.dropna().index].iloc[geneFold_train], normedScores.loc[normedScores.dropna().index].iloc[geneFold_train], fitTable)\n",
    "\n",
    "    transformedParams_test = transformParams(paramTable_trainingGuides.loc[normedScores.dropna().index].iloc[geneFold_test], fitTable, estimators)\n",
    "    \n",
    "    reg = linear_model.ElasticNetCV(l1_ratio=[.5, .75, .9, .99,1], n_jobs=16, max_iter=2000)\n",
    "    \n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    reg.fit(scaler.fit_transform(transformedParams_train), normedScores.loc[normedScores.dropna().index].iloc[geneFold_train])\n",
    "    predictedScores = pd.Series(reg.predict(scaler.transform(transformedParams_test)), index=transformedParams_test.index)\n",
    "    testScores = normedScores.loc[normedScores.dropna().index].iloc[geneFold_test]\n",
    "    \n",
    "\n",
    "   \n",
    "#    print ('Prediction AUC-ROC:', metrics.roc_auc_score((testScores >= .75).values, np.array(predictedScores.values,dtype='float64')))\n",
    "    R2Score = reg.score(scaler.transform(transformedParams_test), testScores)\n",
    "    print ('Prediction R^2:', R2Score )\n",
    "    print ('Regression parameters:', reg.l1_ratio_, reg.alpha_)\n",
    "    coefs.append(pd.DataFrame(zip(*[abs(reg.coef_),reg.coef_]), index = transformedParams_test.columns, columns=['abs','true']))\n",
    "    numFeatures = len(coefs[-1]) - sum(coefs[-1]['abs'] < .00000000001)\n",
    "    print ('Number of features used:', numFeatures)\n",
    "    \n",
    "    \n",
    "    metricTups.append((R2Score,numFeatures,geneFold_train,reg,scaler,(testScores,predictedScores),geneFold_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transformedParams_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort the folds based on R^2 and features and select the top-most for our model\n",
    "metricTupsSorted= sorted(metricTups, key= lambda element: (element[0], element[1]), reverse=True)\n",
    "   \n",
    "\n",
    "geneFold_train = metricTupsSorted[0][2]\n",
    "reg = metricTupsSorted[0][3]\n",
    "scaler = metricTupsSorted[0][4]\n",
    "scoreTups = metricTupsSorted[0][5]\n",
    "geneFold_test= metricTupsSorted[0][6]\n",
    "\n",
    "#just recalculate the estimator and transformedParams_train\n",
    "transformedParams_train, estimators = fitParams(paramTable_trainingGuides.loc[normedScores.dropna().index].iloc[geneFold_train], normedScores.loc[normedScores.dropna().index].iloc[geneFold_train], fitTable)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('R^2 coefficient value selected: ', metricTupsSorted[0][0])\n",
    "print ('Number of Features: ', metricTupsSorted[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the features utilized by model\n",
    "print(transformedParams_train.iloc[:,reg.coef_!=0].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the coefficients of the features calculated by regression algorithm\n",
    "result= filter(lambda x: x != 0, reg.coef_)\n",
    "print (list(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import math\n",
    "\n",
    "dfPlot = transformedParams_train.filter(like='TSS')\n",
    "\n",
    "\n",
    "\n",
    "total_items = len(dfPlot.columns)\n",
    "items_per_row = 3\n",
    "total_rows = math.ceil(total_items / items_per_row)\n",
    "fig = make_subplots(rows=total_rows, cols=items_per_row)\n",
    "cur_row = 1\n",
    "cur_col = 1\n",
    "for index, column in enumerate(dfPlot.columns):\n",
    "    fig.add_trace(go.Box(y=dfPlot[column], name=str(column)), row=cur_row, col=cur_col)\n",
    "    \n",
    "    if cur_col % items_per_row == 0:\n",
    "        cur_col = 1\n",
    "        cur_row = cur_row + 1\n",
    "    else:\n",
    "        cur_col = cur_col + 1\n",
    "    \n",
    "fig.update_layout(height=1000, width=1000,  showlegend=False)\n",
    "fig.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "for t in dfPlot.columns.values:\n",
    "    titles.append(str(t))\n",
    "normedScoresPlot=normedScores.loc[normedScores.dropna().index].iloc[geneFold_train]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import math\n",
    "import numpy as np\n",
    "total_items = len(dfPlot.columns)\n",
    "items_per_row = 3\n",
    "total_rows = math.ceil(total_items / items_per_row)\n",
    "fig = make_subplots(rows=total_rows, cols=items_per_row, subplot_titles=titles)\n",
    "cur_row = 1\n",
    "cur_col = 1\n",
    "for index, column in enumerate(dfPlot.columns):\n",
    "    fig.add_trace(go.Scattergl(x=dfPlot[column], \n",
    "                            y=normedScoresPlot.iloc[:,0], \n",
    "                            mode=\"markers\", \n",
    "                            marker=dict(size=3)), \n",
    "                  row=cur_row, \n",
    "                  col=cur_col)\n",
    "    \n",
    "    intercept = np.poly1d(np.polyfit(dfPlot[column], normedScoresPlot.iloc[:,0], 1))(np.unique(dfPlot[column]))\n",
    "    \n",
    "    fig.add_trace(go.Scatter(x=np.unique(dfPlot[column]), \n",
    "                             y=intercept, \n",
    "                             line=dict(color='red', width=1)), \n",
    "                  row=cur_row, \n",
    "                  col=cur_col)\n",
    "    \n",
    "    if cur_col % items_per_row == 0:\n",
    "        cur_col = 1\n",
    "        cur_row = cur_row + 1\n",
    "    else:\n",
    "        cur_col = cur_col + 1\n",
    "    \n",
    "fig.update_layout(height=1000, width=1000, showlegend=False)\n",
    "print(\"X axis: Distance of guide from TSS, Y axis: Score of the guide\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the best fold\n",
    "#the pickling of the scikit-learn estimators/regressors will allow the model to be reloaded for prediction of other guide designs, \n",
    "#   but will not be compatible across scikit-learn versions, so it is important to preserve the training data and training/test folds\n",
    "\n",
    "import _pickle as cPickle\n",
    "\n",
    "estimatorString = cPickle.dumps((fitTable, estimators, scaler, reg, (geneFold_train, geneFold_test)))\n",
    "with open(PICKLE_FILE,'wb') as outfile:\n",
    "    outfile.write(estimatorString)\n",
    "    \n",
    "#also save the transformed parameters as these can slightly differ based on the automated binning strategy\n",
    "\n",
    "transformedParamsTrainHead = cPickle.dumps(transformedParams_train.head())\n",
    "\n",
    "with open(TRANSFORMED_PARAM_HEADER,'wb') as paramfile:\n",
    "    paramfile.write(transformedParamsTrainHead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. Applying machine learning model to predict sgRNA activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#starting from a new session for demonstration purposes:\n",
    "%run sgRNA_learning.py\n",
    "import _pickle as cPickle\n",
    "#import cPickle\n",
    "\n",
    "#load tssTable, p1p2Table, genome sequence, chromatin data\n",
    "tssTable = pd.read_csv(TSS_TABLE_PATH,sep='\\t', index_col=[0,1])\n",
    "\n",
    "p1p2Table = pd.read_csv(P1P2_TABLE_PATH,sep='\\t', header=0, index_col=[0,1])\n",
    "p1p2Table['primary TSS'] = p1p2Table['primary TSS'].apply(lambda tupString: (int(float(tupString.strip('()').split(', ')[0])), int(float(tupString.strip('()').split(', ')[1]))))\n",
    "p1p2Table['secondary TSS'] = p1p2Table['secondary TSS'].apply(lambda tupString: (int(float(tupString.strip('()').split(', ')[0])),int(float(tupString.strip('()').split(', ')[1]))))\n",
    "\n",
    "genomeDict = loadGenomeAsDict(FASTA_FILE_OF_GENOME)\n",
    "\n",
    "bwhandleDict = {'dnase':BigWigFile(open(LARGE_FILE_DIR + 'large_data_files/wgEncodeOpenChromDnaseK562BaseOverlapSignalV2.bigWig','rb')),\n",
    "'faire':BigWigFile(open(LARGE_FILE_DIR + 'large_data_files/wgEncodeOpenChromFaireK562Sig.bigWig','rb')),\n",
    "'mnase':BigWigFile(open(LARGE_FILE_DIR + 'large_data_files/wgEncodeSydhNsomeK562Sig.bigWig','rb'))}\n",
    "\n",
    "#load sgRNA prediction model saved after the parameter fitting step\n",
    "with open(PICKLE_FILE,'rb') as infile:\n",
    "    fitTable, estimators, scaler, reg, (geneFold_train, geneFold_test) = cPickle.load(infile)\n",
    "    \n",
    "#transformedParamHeader = pd.read_csv(TRANSFORMED_PARAM_HEADER,sep='\\t')\n",
    "\n",
    "#iGEM read the binary file\n",
    "#transformedParamHeader = pd.read_csv(TRANSFORMED_PARAM_HEADER,sep='\\t')\n",
    "\n",
    "with open(TRANSFORMED_PARAM_HEADER,'rb') as paraminfile:\n",
    "    transformedParamHeader = cPickle.load(paraminfile)\n",
    "\n",
    "transformedParams_train = transformedParamHeader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find all sgRNAs in genomic regions of interest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For IGEM for now try to score the Gene that we excluded from the training data set\n",
    "\n",
    "#Select excluded gene fron training set\n",
    "libraryTable_new = libraryTable_research\n",
    "libraryTable_new = libraryTable_research[ libraryTable_research['gene'] == GENE_TO_EXCLUDE ]\n",
    "\n",
    "#libraryTable_subset = libraryTable_research\n",
    "sgInfoTable_new = parseAllSgIds(libraryTable_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "libraryTable_new.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting sgRNA activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate parameters for new sgRNAs\n",
    "paramTable_new = generateTypicalParamTableEx(libraryTable_new, sgInfoTable_new, tssTable, p1p2Table, genomeDict, bwhandleDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform and predict scores according to sgRNA prediction model\n",
    "transformedParams_new = transformParams(paramTable_new, fitTable, estimators)\n",
    "\n",
    "#reconcile any differences in column headers generated by automated binning\n",
    "colTups = []\n",
    "for (l1, l2), col in transformedParams_new.iteritems():\n",
    "    colTups.append((l1,str(l2)))\n",
    "transformedParams_new.columns = pd.MultiIndex.from_tuples(colTups)\n",
    "#iGEM in python 3 .loc with missing column headers is giving issues. So changing it to reindex # can use transformedParams_train if running sequentially otherwise use transformedParamHeader after running above step\n",
    "predictedScores_new = pd.Series(reg.predict(scaler.transform(transformedParams_new.reindex(columns=transformedParams_train.columns).fillna(0).values)), index=transformedParams_new.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedScores_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Run this cell to create a comparison matrix of the scores measured in the lab: Weissman v iGEM model\n",
    "compData = df_research\n",
    "#compData=compData.set_index('sgID')\n",
    "compData = compData.loc[predictedScores_new.index]\n",
    "compData['Measured by Dr. Perli']= 1-compData['Measured by Dr. Perli']\n",
    "compData['Dr. Perli rank']=compData['Measured by Dr. Perli'].rank(ascending=False)\n",
    "compData['iGem score']=predictedScores_new\n",
    "compData['iGem rank']=compData['iGem score'].rank(ascending=False)\n",
    "compData.filter(regex='sgId')\n",
    "compData= compData.filter(regex='gene|.*rank|.*score|Me.*|.*rank')\n",
    "compData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_tweedie_deviance\n",
    "\n",
    "lab_scores =compData['Measured by Dr. Perli'].values\n",
    "weissman_scores = compData['predicted score'].values\n",
    "iGem_scores = compData['iGem score'].values\n",
    "\n",
    "\n",
    "print ('Compare Deviance of Weissman and iGem From Lab Scores Lower the value, better the model\\n')\n",
    "print ('Weismann: ', mean_tweedie_deviance(lab_scores, weissman_scores,power=0))\n",
    "print ('iGEM:     ', mean_tweedie_deviance(lab_scores, iGem_scores,power=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedScores_new.to_csv(PREDICTED_SCORE_TABLE, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Construct sgRNA libraries\n",
    "## Score sgRNAs for off-target potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are many ways to score sgRNAs as off-target; below is one listed one method that is simple and flexible,\n",
    "#but ignores gapped alignments, alternate PAMs, and uses bowtie which may not be maximally sensitive in all cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iGEM the sequence length can be greather 22. So just added more pluses and adjusteed its length equal to sequence length.\n",
    "#iGEM Revisit to fix phred length properly.\n",
    "\n",
    "#output all sequences to a temporary FASTQ file for running bowtie alignment\n",
    "def outputTempBowtieFastq(libraryTable, outputFileName):\n",
    "    phredString = 'I4!=======44444++++++++++++++++' #weighting for how impactful mismatches are along sgRNA sequence \n",
    "    with open(outputFileName,'w') as outfile:\n",
    "        for name, row in libraryTable.iterrows():\n",
    "            outfile.write('@' + name + '\\n')\n",
    "            outfile.write('CCN' + str(Seq.Seq(row['sequence'][1:]).reverse_complement()) + '\\n')\n",
    "            outfile.write('+\\n')\n",
    "            outfile.write(phredString[0:3+len(str(Seq.Seq(row['sequence'][1:]).reverse_complement()))] + '\\n')\n",
    "            \n",
    "outputTempBowtieFastq(libraryTable_new, TEMP_FASTQ_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "fqFile = TEMP_FASTQ_FILE\n",
    "\n",
    "#specifying a list of parameters to run bowtie with\n",
    "#each tuple contains\n",
    "# *the mismatch threshold below which a site is considered a potential off-target (higher is more stringent)\n",
    "# *the number of sites allowed (1 is minimum since each sgRNA should have one true site in genome)\n",
    "# *the genome index against which to align the sgRNA sequences; these can be custom built to only consider sites near TSSs\n",
    "# *a name for the bowtie run to create appropriately named output files\n",
    "\n",
    "#iGEM\n",
    "#alignmentList = [(39,1,'~/indices/hg19.ensemblTSSflank500b','39_nearTSS'),\n",
    "#                (31,1,'~/indices/hg19.ensemblTSSflank500b','31_nearTSS'),\n",
    "#                (21,1,'~/indices/hg19.maskChrMandPAR','21_genome'),\n",
    "#                (31,2,'~/indices/hg19.ensemblTSSflank500b','31_2_nearTSS'),\n",
    "#                (31,3,'~/indices/hg19.ensemblTSSflank500b','31_3_nearTSS')]\n",
    "\n",
    "#iGEM   ChrM and PAR are vary small part of hg19. So running alignment for entire genome.\n",
    "\n",
    "alignmentList = [(39,1,LARGE_FILE_DIR+'large_data_files/indices/hg19.ensemblTSSflank500b','39_nearTSS'),\n",
    "                (31,1,LARGE_FILE_DIR+'large_data_files/indices/hg19.ensemblTSSflank500b','31_nearTSS'),\n",
    "                #(21,1,'/data/large_data_files/indices/hg19_maskChrMandPAR','21_genome'),\n",
    "                (21,1,LARGE_FILE_DIR+'large_data_files/indices/hg19','21_genome'),\n",
    "                (31,2,LARGE_FILE_DIR+'large_data_files/indices/hg19.ensemblTSSflank500b','31_2_nearTSS'),\n",
    "                (31,3,LARGE_FILE_DIR+'large_data_files/indices/hg19.ensemblTSSflank500b','31_3_nearTSS')]\n",
    "import os\n",
    "import errno\n",
    "if not os.path.exists('bowtie_output'):\n",
    "    os.makedirs('bowtie_output')\n",
    "\n",
    "alignmentColumns = []\n",
    "for btThreshold, mflag, bowtieIndex, runname in alignmentList:\n",
    "\n",
    "    alignedFile = 'bowtie_output/' + runname + '_aligned.txt'\n",
    "    unalignedFile = 'bowtie_output/' + runname + '_unaligned.fq'\n",
    "    maxFile = 'bowtie_output/' + runname + '_max.fq'\n",
    "    \n",
    "    bowtieString = 'bowtie -n 3 -l 15 -e '+str(btThreshold)+' -m ' + str(mflag) + ' --nomaqround -a --tryhard -p 16 --chunkmbs 256 ' + bowtieIndex + ' --suppress 5,6,7 --un ' + unalignedFile + ' --max ' + maxFile + ' '+ ' -q '+fqFile+' '+ alignedFile\n",
    "    print (bowtieString)\n",
    "    print (subprocess.call(bowtieString, shell=True))\n",
    "\n",
    "    #parse through the file of sgRNAs that exceeded \"m\", the maximum allowable alignments, and mark \"True\" any that are found\n",
    "    sgsAligning = set()\n",
    "    \n",
    "    try:\n",
    "        with open(maxFile) as infile:\n",
    "            sgsAligning = set()\n",
    "            for i, line in enumerate(infile):\n",
    "                if i%4 == 0: #id line\n",
    "                    sgsAligning.add(line.strip()[1:])\n",
    "    except OSError as e:\n",
    "        if e.errno != errno.ENOENT:\n",
    "            raise\n",
    "\n",
    "    alignmentColumns.append(libraryTable_new.apply(lambda row: row.name in sgsAligning, axis=1))\n",
    "#iGEM zip is an object in python3    \n",
    "#collate results into a table, and flip the boolean values to yield the sgRNAs that passed filter as True\n",
    "alignmentTable = pd.concat(alignmentColumns,axis=1, keys=list(zip(*alignmentList))[3]).ne(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick the top sgRNAs for a library, given predicted activity scores and off-target filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all generated data into one master table\n",
    "predictedScores_new.name = 'predicted score'\n",
    "v2Table = pd.concat((libraryTable_new, predictedScores_new, alignmentTable, sgInfoTable_new), axis=1, keys=['library table v2', 'predicted score', 'off-target filters', 'sgRNA info'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#for our pCRISPRi/a-v2 vector, we append flanking sequences to each sgRNA sequence for cloning and require the oligo to contain\n",
    "#exactly 1 BstXI and BlpI site each for cloning, and exactly 0 SbfI sites for sequencing sample preparation\n",
    "restrictionSites = {re.compile('CCA......TGG'):1,\n",
    "                   re.compile('GCT.AGC'):1,\n",
    "                   re.compile('CCTGCAGG'):0}\n",
    "\n",
    "def matchREsites(sequence, REdict):\n",
    "    seq = sequence.upper()\n",
    "#iGEM in python 3 dict.iteritems is not present. So replace with dict.items        \n",
    "#    for resite, numMatchesExpected in restrictionSites.iteritems():\n",
    "    for resite, numMatchesExpected in restrictionSites.items():\n",
    "        if len(resite.findall(seq)) != numMatchesExpected:\n",
    "            return False\n",
    "        \n",
    "    return True\n",
    "\n",
    "def checkOverlaps(leftPosition, acceptedLeftPositions, nonoverlapMin):\n",
    "    for pos in acceptedLeftPositions:\n",
    "        if abs(pos - leftPosition) < nonoverlapMin:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flanking sequences\n",
    "upstreamConstant = 'CCACCTTGTTG'\n",
    "downstreamConstant = 'GTTTAAGAGCTAAGCTG'\n",
    "\n",
    "#minimum overlap between two sgRNAs targeting the same TSS\n",
    "nonoverlapMin = 3\n",
    "\n",
    "#number of sgRNAs to pick per gene/TSS\n",
    "sgRNAsToPick = 10\n",
    "\n",
    "#iGEM TODO need to enable 21_genome as well. temporarily disabled\n",
    "\n",
    "#list of off-target filter (or combinations of filters) levels, matching the names in the alignment table above\n",
    "offTargetLevels = [['31_nearTSS', '21_genome'],\n",
    "                  ['31_nearTSS'],\n",
    "                  ['21_genome'],\n",
    "                  ['31_2_nearTSS'],\n",
    "                  ['31_3_nearTSS']]\n",
    "\n",
    "#offTargetLevels = [ ['31_nearTSS'],\n",
    "#                  ['31_2_nearTSS'],\n",
    "#                  ['31_3_nearTSS']]\n",
    "\n",
    "\n",
    "#for each gene/TSS, go through each sgRNA in descending order of predicted score\n",
    "#if an sgRNA passes the restriction site, overlap, and off-target filters, accept it into the library\n",
    "#if the number of sgRNAs accepted is less than sgRNAsToPick, reduce off-target stringency by one and continue\n",
    "v2Groups = v2Table.groupby([('library table v2','gene'),('library table v2','transcripts')])\n",
    "newSgIds = []\n",
    "unfinishedTss = []\n",
    "for (gene, transcript), group in v2Groups:\n",
    "    geneSgIds = []\n",
    "    geneLeftPositions = []\n",
    "    empiricalSgIds = dict()\n",
    "    \n",
    "    stringency = 0\n",
    "#iGEM use sort_values instead of sort    \n",
    "    while len(geneSgIds) < sgRNAsToPick and stringency < len(offTargetLevels):\n",
    "        for sgId_v2, row in group.sort_values(('predicted score','predicted score'), ascending=False).iterrows():\n",
    "            oligoSeq = upstreamConstant + row[('library table v2','sequence')] + downstreamConstant\n",
    "            leftPos = row[('sgRNA info', 'position')] - (23 if row[('sgRNA info', 'strand')] == '-' else 0)\n",
    "            if len(geneSgIds) < sgRNAsToPick and row['off-target filters'].loc[offTargetLevels[stringency]].all() \\\n",
    "                and matchREsites(oligoSeq, restrictionSites) \\\n",
    "                and checkOverlaps(leftPos, geneLeftPositions, nonoverlapMin):\n",
    "                geneSgIds.append((sgId_v2,\n",
    "                                  gene,transcript,\n",
    "                                  row[('library table v2','sequence')], oligoSeq,\n",
    "                                  row[('predicted score','predicted score')], np.nan,\n",
    "                                 stringency))\n",
    "                geneLeftPositions.append(leftPos)\n",
    "                \n",
    "        stringency += 1\n",
    "            \n",
    "    if len(geneSgIds) < sgRNAsToPick:\n",
    "        unfinishedTss.append((gene, transcript)) #if the number of accepted sgRNAs is still less than sgRNAsToPick, discard gene\n",
    "    else:\n",
    "        newSgIds.extend(geneSgIds)\n",
    "        \n",
    "libraryTable_complete = pd.DataFrame(newSgIds, columns = ['sgID', 'gene', 'transcript','protospacer sequence', 'oligo sequence',\n",
    " 'predicted score', 'empirical score', 'off-target stringency']).set_index('sgID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfinishedTss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of sgRNAs accepted at each stringency level\n",
    "#iGEM newLibraryTable is not defined\n",
    "#newLibraryTable.groupby('off-target stringency').agg(len).iloc[:,0]\n",
    "libraryTable_complete.groupby('off-target stringency').agg(len).iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of TSSs with fewer than required number of sgRNAs (and thus not included in the library)\n",
    "print (len(unfinishedTss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "libraryTable_complete"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
