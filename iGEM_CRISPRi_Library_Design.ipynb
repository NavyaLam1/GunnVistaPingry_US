{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Learning sgRNA predictors from empirical data\n",
    "    * Load scripts and empirical data\n",
    "    * Generate TSS annotation using FANTOM dataset\n",
    "    * Calculate parameters for empirical sgRNAs\n",
    "    * Fit parameters\n",
    "2. Applying machine learning model to predict sgRNA activity\n",
    "    * Find all sgRNAs in genomic regions of interest \n",
    "    * Predicting sgRNA activity\n",
    "3. Construct sgRNA libraries\n",
    "    * Score sgRNAs for off-target potential\n",
    "* Pick the top sgRNAs for a library, given predicted activity scores and off-target filtering\n",
    "* Design negative controls matching the base composition of the library\n",
    "* Finalizing library design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Learning sgRNA predictors from empirical data\n",
    "## Load scripts and empirical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "LARGE_FILE_DIR= '../../../../data/'\n",
    "\n",
    "FASTA_FILE_OF_GENOME= LARGE_FILE_DIR + 'large_data_files/hg19.fa'\n",
    "GTF_FILE_FROM_GENCODE = LARGE_FILE_DIR + 'large_data_files/gencode.v19.annotation.gtf'\n",
    "\n",
    "TSS_TABLE_PATH='data_files/human_tssTable.txt'\n",
    "P1P2_TABLE_PATH='data_files/human_p1p2Table.txt'\n",
    "\n",
    "DISCRIMANT_THRESHOLD_eg20=20\n",
    "FANTOM_TSS_ANNOTATION_BED= LARGE_FILE_DIR + 'large_data_files/TSS_human.sorted.bed.gz'\n",
    "HGNC_SYMBOL_LOOKUP_TABLE= LARGE_FILE_DIR + 'large_data_files/hgnc_complete_set_2020-08-01.txt'\n",
    "\n",
    "\n",
    "LIBRARY_TABLE_PATH='data_files/CRISPRi_trainingdata_libraryTable.txt'\n",
    "SGINFO_TABLE_PATH='data_files/CRISPRi_trainingdata_sgRNAInfoTable.txt'\n",
    "\n",
    "\n",
    "IGEM_EXCEL_FILE= LARGE_FILE_DIR + 'large_data_files/Sam_Perli_qRT_pcr_data_per_gene.xlsx'\n",
    "\n",
    "\n",
    "PICKLE_FILE = 'igem_v1_esitmator'\n",
    "TRANSFORMED_PARAM_HEADER='igem_v1_transformed_param_header'\n",
    "PREDICTED_SCORE_TABLE='igem_v1_predicted_score_table'\n",
    "TEMP_FASTQ_FILE='igem_v1_temp_fastq_file'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#TSS_TABLE_BASED_ON_ENSEMBL ='data_files/human_tssTable.txt\n",
    "#FANTOM_TSS_ANNOTATION_BED\n",
    "\n",
    "\n",
    "%run sgRNA_learning.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.read_excel(IGEM_EXCEL_FILE, index_col=0) \n",
    "df=pd.read_excel(open(IGEM_EXCEL_FILE, 'rb'),\n",
    "              sheet_name='Sheet1')  \n",
    "df =df.dropna( subset = ['sgID'])\n",
    "df.set_index('sgID')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n",
    "libraryTable_research = df[['sgID','gene','transcript','protospacer sequence']]\n",
    "#libraryTable_research.rename(columns={'protospacer sequence':'sequence'})\n",
    "libraryTable_research= libraryTable_research.loc[1:,].set_index('sgID').rename(columns={'protospacer sequence':'sequence'})\n",
    "libraryTable_research\n",
    "GENE_TO_EXCLUDE= 'PRRC2A'\n",
    "\n",
    "#Select all genes except last one\n",
    "libraryTable_subset = libraryTable_research\n",
    "libraryTable_subset = libraryTable_research[ libraryTable_research['gene'] != GENE_TO_EXCLUDE ]\n",
    "sgInfoTable = parseAllSgIds(libraryTable_subset)\n",
    "\n",
    "normedScores = df[['sgID','gene','Measured by Dr. Perli']].loc[1:,]\n",
    "normedScores = normedScores [ normedScores['gene'] != GENE_TO_EXCLUDE]\n",
    "\n",
    "#normedScores = normedScores [['sgID','Measured by Dr. Perli']].set_index('sgID').rename(columns={'Measured by Dr. Perli':''})\n",
    "#normedScores['Measured by Dr. Perli']= (-1)*normedScores['Measured by Dr. Perli']\n",
    "normedScores['Measured by Dr. Perli']= np.log10(1/normedScores['Measured by Dr. Perli'])\n",
    "normedScores = normedScores [['sgID','Measured by Dr. Perli']].set_index('sgID').rename(columns={'Measured by Dr. Perli':''})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgInfoTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(normedScores.head())\n",
    "#normedScores.describe()\n",
    "normedScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genomeDict = loadGenomeAsDict(FASTA_FILE_OF_GENOME)\n",
    "gencodeData = loadGencodeData(GTF_FILE_FROM_GENCODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (libraryTable_subset.count())\n",
    "print (libraryTable_subset.columns)\n",
    "print (libraryTable_subset.head())\n",
    "\n",
    "print (sgInfoTable.count())\n",
    "print (sgInfoTable.columns)\n",
    "print (sgInfoTable.head())\n",
    "#print (phenotypeTable.columns)\n",
    "#print (geneFoldList)\n",
    "\n",
    "print (normedScores.count())\n",
    "print (normedScores.columns)\n",
    "print (normedScores.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate TSS annotation using FANTOM dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "#save tables for downstream use\n",
    "#tssTable = pd.read_csv(TSS_TABLE_PATH,sep='\\t', index_col=range(2))\n",
    "tssTable = pd.read_csv(TSS_TABLE_PATH,sep='\\t', index_col=[0,1])\n",
    "\n",
    "\n",
    "#p1p2Table = pd.read_csv(P1P2_TABLE_PATH,sep='\\t', header=0, index_col=range(2))\n",
    "p1p2Table = pd.read_csv(P1P2_TABLE_PATH,sep='\\t', header=0, index_col=[0,1], converters={\"primary TSS\": ast.literal_eval, \"secondary TSS\": ast.literal_eval})\n",
    "\n",
    "tssTable.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Calculate parameters for empirical sgRNAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load bigwig files for any chromatin data of interest\n",
    "#bwhandleDict = {'dnase':BigWigFile(open('ENCODE_data/wgEncodeOpenChromDnaseK562BaseOverlapSignalV2.bigWig','rb')),\n",
    "#'faire':BigWigFile(open('ENCODE_data/wgEncodeOpenChromFaireK562Sig.bigWig','rb')),\n",
    "#'mnase':BigWigFile(open('ENCODE_data/wgEncodeSydhNsomeK562Sig.bigWig','rb'))}\n",
    "bwhandleDict = {'dnase':BigWigFile(open(LARGE_FILE_DIR + 'large_data_files/wgEncodeOpenChromDnaseK562BaseOverlapSignalV2.bigWig','rb')),\n",
    "'faire':BigWigFile(open(LARGE_FILE_DIR + 'large_data_files/wgEncodeOpenChromFaireK562Sig.bigWig','rb')),\n",
    "'mnase':BigWigFile(open(LARGE_FILE_DIR + 'large_data_files/wgEncodeSydhNsomeK562Sig.bigWig','rb'))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import pdb\n",
    "#pdb.set_trace()\n",
    "#paramTable_trainingGuides = generateTypicalParamTable(libraryTable_subset,sgInfoTable, tssTable, p1p2Table, genomeDict, bwhandleDict)\n",
    "paramTable_trainingGuides = generateTypicalParamTableEx(libraryTable_subset,sgInfoTable, tssTable, p1p2Table, genomeDict, bwhandleDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramTable_trainingGuides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramTable_trainingGuides.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( len(paramTable_trainingGuides.columns) )\n",
    "paramTable_trainingGuides.columns\n",
    "paramTable_trainingGuides.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#populate table of fitting parameters\n",
    "typeList = ['binnable_onehot', \n",
    "            'continuous', 'continuous', 'continuous', 'continuous',\n",
    "            'continuous', 'continuous', 'continuous', 'continuous',\n",
    "            'binnable_onehot','binnable_onehot','binnable_onehot','binnable_onehot',\n",
    "            'binnable_onehot','binnable_onehot','binnable_onehot','binnable_onehot','binnable_onehot','binnable_onehot','binnable_onehot',\n",
    "            'binary']\n",
    "typeList.extend(['binary']*160)\n",
    "typeList.extend(['binary']*(16*38))\n",
    "typeList.extend(['binnable_onehot']*3)\n",
    "typeList.extend(['binnable_onehot']*2)\n",
    "typeList.extend(['binary']*18)\n",
    "fitTable = pd.DataFrame(typeList, index=paramTable_trainingGuides.columns, columns=['type'])\n",
    "#we may have to reduce min edge data from 50\n",
    "MIN_EDGE_DATA=10\n",
    "fitparams =[{'bin width':1, 'min edge data':MIN_EDGE_DATA, 'bin function':np.median},\n",
    "            {'C':[.01,.05, .1,.5], 'gamma':[.000001, .00005,.0001,.0005]},\n",
    "            {'C':[.01,.05, .1,.5], 'gamma':[.000001, .00005,.0001,.0005]},\n",
    "            {'C':[.01,.05, .1,.5], 'gamma':[.000001, .00005,.0001,.0005]},\n",
    "            {'C':[.01,.05, .1,.5], 'gamma':[.000001, .00005,.0001,.0005]},\n",
    "            {'C':[.01,.05, .1,.5], 'gamma':[.000001, .00005,.0001,.0005]},\n",
    "            {'C':[.01,.05, .1,.5], 'gamma':[.000001, .00005,.0001,.0005]},\n",
    "            {'C':[.01,.05, .1,.5], 'gamma':[.000001, .00005,.0001,.0005]},\n",
    "            {'C':[.01,.05, .1,.5], 'gamma':[.000001, .00005,.0001,.0005]},\n",
    "            {'bin width':1, 'min edge data':MIN_EDGE_DATA, 'bin function':np.median},\n",
    "            {'bin width':1, 'min edge data':MIN_EDGE_DATA, 'bin function':np.median},\n",
    "            {'bin width':1, 'min edge data':MIN_EDGE_DATA, 'bin function':np.median},\n",
    "            {'bin width':1, 'min edge data':MIN_EDGE_DATA, 'bin function':np.median},\n",
    "            {'bin width':.1, 'min edge data':MIN_EDGE_DATA, 'bin function':np.median},\n",
    "            {'bin width':.1, 'min edge data':MIN_EDGE_DATA, 'bin function':np.median},\n",
    "            {'bin width':.1, 'min edge data':MIN_EDGE_DATA, 'bin function':np.median},\n",
    "            {'bin width':.1, 'min edge data':MIN_EDGE_DATA, 'bin function':np.median},\n",
    "            {'bin width':.1, 'min edge data':MIN_EDGE_DATA, 'bin function':np.median},\n",
    "            {'bin width':.1, 'min edge data':MIN_EDGE_DATA, 'bin function':np.median},\n",
    "            {'bin width':.1, 'min edge data':MIN_EDGE_DATA, 'bin function':np.median},\n",
    "            dict()]\n",
    "fitparams.extend([dict()]*160)\n",
    "fitparams.extend([dict()]*(16*38))\n",
    "fitparams.extend([\n",
    "            {'bin width':.15, 'min edge data':MIN_EDGE_DATA, 'bin function':np.median},\n",
    "            {'bin width':.15, 'min edge data':MIN_EDGE_DATA, 'bin function':np.median},\n",
    "            {'bin width':.15, 'min edge data':MIN_EDGE_DATA, 'bin function':np.median}])\n",
    "fitparams.extend([\n",
    "            {'bin width':2, 'min edge data':MIN_EDGE_DATA, 'bin function':np.median},\n",
    "            {'bin width':2, 'min edge data':MIN_EDGE_DATA, 'bin function':np.median}])\n",
    "fitparams.extend([dict()]*18)\n",
    "fitTable['params'] = fitparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramTable_trainingGuides.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pdb\n",
    "#pdb.set_trace()\n",
    "#Train set includes all genes except 1 May have to retry this step and training multiple times until we get an estimator with atleast one feature.\n",
    "# \"Number of features used: \" printed at the end of below training cell should a value greather than zero.\n",
    "geneFoldList = getGeneFoldsEx(libraryTable_subset, 12, transcripts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geneFoldList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pdb\n",
    "#pdb.set_trace()\n",
    "\n",
    "#for each fold, fit parameters to training folds and measure ROC on test fold\n",
    "coefs = []\n",
    "scoreTups = []\n",
    "transformedParamTups = []\n",
    "metricTups = []\n",
    "\n",
    "for geneFold_train, geneFold_test in geneFoldList:\n",
    "\n",
    "    transformedParams_train, estimators = fitParams(paramTable_trainingGuides.loc[normedScores.dropna().index].iloc[geneFold_train], normedScores.loc[normedScores.dropna().index].iloc[geneFold_train], fitTable)\n",
    "\n",
    "    transformedParams_test = transformParams(paramTable_trainingGuides.loc[normedScores.dropna().index].iloc[geneFold_test], fitTable, estimators)\n",
    "    \n",
    "    reg = linear_model.ElasticNetCV(l1_ratio=[.5, .75, .9, .99,1], n_jobs=16, max_iter=2000)\n",
    "    \n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    reg.fit(scaler.fit_transform(transformedParams_train), normedScores.loc[normedScores.dropna().index].iloc[geneFold_train])\n",
    "    predictedScores = pd.Series(reg.predict(scaler.transform(transformedParams_test)), index=transformedParams_test.index)\n",
    "    testScores = normedScores.loc[normedScores.dropna().index].iloc[geneFold_test]\n",
    "    \n",
    "    transformedParamTups.append((scaler.transform(transformedParams_train),scaler.transform(transformedParams_test)))\n",
    "    scoreTups.append((testScores, predictedScores))\n",
    "    \n",
    "\n",
    "\n",
    "#  for now ignore one class errors.....    \n",
    "#    print ('Prediction AUC-ROC:', metrics.roc_auc_score((testScores >= .75).values, np.array(predictedScores.values,dtype='float64')))\n",
    "    R2Score = reg.score(scaler.transform(transformedParams_test), testScores)\n",
    "    print ('Prediction R^2:', R2Score )\n",
    "    print ('Regression parameters:', reg.l1_ratio_, reg.alpha_)\n",
    "    coefs.append(pd.DataFrame(zip(*[abs(reg.coef_),reg.coef_]), index = transformedParams_test.columns, columns=['abs','true']))\n",
    "    numFeatures = len(coefs[-1]) - sum(coefs[-1]['abs'] < .00000000001)\n",
    "    print ('Number of features used:', numFeatures)\n",
    "    \n",
    "    \n",
    "    metricTups.append((R2Score,numFeatures,geneFold_train,reg,scaler,(testScores,predictedScores),geneFold_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transformedParams_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricTupsSorted= sorted(metricTups, key= lambda element: (element[0], element[1]), reverse=True)\n",
    "\n",
    "#metricTups.append((R2Score,numFeatures,geneFold_train,reg,scaler,(testScores,predictedScores),geneFold_test)\n",
    "   \n",
    "\n",
    "geneFold_train = metricTupsSorted[0][2]\n",
    "reg = metricTupsSorted[0][3]\n",
    "scaler = metricTupsSorted[0][4]\n",
    "scoreTups = metricTupsSorted[0][5]\n",
    "geneFold_test= metricTupsSorted[0][6]\n",
    "\n",
    "#just recalculate the estimator and transformedParams_train for now\n",
    "transformedParams_train, estimators = fitParams(paramTable_trainingGuides.loc[normedScores.dropna().index].iloc[geneFold_train], normedScores.loc[normedScores.dropna().index].iloc[geneFold_train], fitTable)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import math\n",
    "\n",
    "df = transformedParams_train.filter(like='TSS')\n",
    "\n",
    "\n",
    "\n",
    "total_items = len(df.columns)\n",
    "items_per_row = 3\n",
    "total_rows = math.ceil(total_items / items_per_row)\n",
    "fig = make_subplots(rows=total_rows, cols=items_per_row)\n",
    "cur_row = 1\n",
    "cur_col = 1\n",
    "for index, column in enumerate(df.columns):\n",
    "    fig.add_trace(go.Box(y=df[column], name=str(column)), row=cur_row, col=cur_col)\n",
    "    \n",
    "    if cur_col % items_per_row == 0:\n",
    "        cur_col = 1\n",
    "        cur_row = cur_row + 1\n",
    "    else:\n",
    "        cur_col = cur_col + 1\n",
    "    \n",
    "fig.update_layout(height=1000, width=1000,  showlegend=False)\n",
    "fig.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "for t in df.columns.values:\n",
    "    titles.append(str(t))\n",
    "normedScores=normedScores.loc[normedScores.dropna().index].iloc[geneFold_train]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import math\n",
    "import numpy as np\n",
    "total_items = len(df.columns)\n",
    "items_per_row = 3\n",
    "total_rows = math.ceil(total_items / items_per_row)\n",
    "fig = make_subplots(rows=total_rows, cols=items_per_row, subplot_titles=titles)\n",
    "cur_row = 1\n",
    "cur_col = 1\n",
    "for index, column in enumerate(df.columns):\n",
    "    fig.add_trace(go.Scattergl(x=df[column], \n",
    "                            y=normedScores.iloc[:,0], \n",
    "                            mode=\"markers\", \n",
    "                            marker=dict(size=3)), \n",
    "                  row=cur_row, \n",
    "                  col=cur_col)\n",
    "    \n",
    "    intercept = np.poly1d(np.polyfit(df[column], normedScores.iloc[:,0], 1))(np.unique(df[column]))\n",
    "    \n",
    "    fig.add_trace(go.Scatter(x=np.unique(df[column]), \n",
    "                             y=intercept, \n",
    "                             line=dict(color='red', width=1)), \n",
    "                  row=cur_row, \n",
    "                  col=cur_col)\n",
    "    \n",
    "    if cur_col % items_per_row == 0:\n",
    "        cur_col = 1\n",
    "        cur_row = cur_row + 1\n",
    "    else:\n",
    "        cur_col = cur_col + 1\n",
    "    \n",
    "fig.update_layout(height=1000, width=1000, showlegend=False)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#can select an arbitrary fold (as shown here simply the last one tested) to save state for reproducing estimators later\n",
    "#the pickling of the scikit-learn estimators/regressors will allow the model to be reloaded for prediction of other guide designs, \n",
    "#   but will not be compatible across scikit-learn versions, so it is important to preserve the training data and training/test folds\n",
    "#import cPickle\n",
    "import _pickle as cPickle\n",
    "\n",
    "estimatorString = cPickle.dumps((fitTable, estimators, scaler, reg, (geneFold_train, geneFold_test)))\n",
    "with open(PICKLE_FILE,'wb') as outfile:\n",
    "    outfile.write(estimatorString)\n",
    "    \n",
    "#also save the transformed parameters as these can slightly differ based on the automated binning strategy\n",
    "\n",
    "#transformedParams_train.head().to_csv(TRANSFORMED_PARAM_HEADER,sep='\\t')\n",
    "#iGEM  instaed of csv write the head in binary format so that it can be read properly. There were \n",
    "\n",
    "transformedParamsTrainHead = cPickle.dumps(transformedParams_train.head())\n",
    "\n",
    "with open(TRANSFORMED_PARAM_HEADER,'wb') as paramfile:\n",
    "    paramfile.write(transformedParamsTrainHead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. Applying machine learning model to predict sgRNA activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#starting from a new session for demonstration purposes:\n",
    "%run sgRNA_learning.py\n",
    "import _pickle as cPickle\n",
    "#import cPickle\n",
    "\n",
    "#load tssTable, p1p2Table, genome sequence, chromatin data\n",
    "tssTable = pd.read_csv(TSS_TABLE_PATH,sep='\\t', index_col=[0,1])\n",
    "\n",
    "p1p2Table = pd.read_csv(P1P2_TABLE_PATH,sep='\\t', header=0, index_col=[0,1])\n",
    "p1p2Table['primary TSS'] = p1p2Table['primary TSS'].apply(lambda tupString: (int(float(tupString.strip('()').split(', ')[0])), int(float(tupString.strip('()').split(', ')[1]))))\n",
    "p1p2Table['secondary TSS'] = p1p2Table['secondary TSS'].apply(lambda tupString: (int(float(tupString.strip('()').split(', ')[0])),int(float(tupString.strip('()').split(', ')[1]))))\n",
    "\n",
    "genomeDict = loadGenomeAsDict(FASTA_FILE_OF_GENOME)\n",
    "\n",
    "bwhandleDict = {'dnase':BigWigFile(open(LARGE_FILE_DIR + 'large_data_files/wgEncodeOpenChromDnaseK562BaseOverlapSignalV2.bigWig','rb')),\n",
    "'faire':BigWigFile(open(LARGE_FILE_DIR + 'large_data_files/wgEncodeOpenChromFaireK562Sig.bigWig','rb')),\n",
    "'mnase':BigWigFile(open(LARGE_FILE_DIR + 'large_data_files/wgEncodeSydhNsomeK562Sig.bigWig','rb'))}\n",
    "\n",
    "#load sgRNA prediction model saved after the parameter fitting step\n",
    "with open(PICKLE_FILE,'rb') as infile:\n",
    "    fitTable, estimators, scaler, reg, (geneFold_train, geneFold_test) = cPickle.load(infile)\n",
    "    \n",
    "#transformedParamHeader = pd.read_csv(TRANSFORMED_PARAM_HEADER,sep='\\t')\n",
    "\n",
    "#iGEM read the binary file\n",
    "#transformedParamHeader = pd.read_csv(TRANSFORMED_PARAM_HEADER,sep='\\t')\n",
    "\n",
    "with open(TRANSFORMED_PARAM_HEADER,'rb') as paraminfile:\n",
    "    transformedParamHeader = cPickle.load(paraminfile)\n",
    "\n",
    "transformedParams_train = transformedParamHeader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find all sgRNAs in genomic regions of interest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For IGEM for now try to score the training data itself.\n",
    "#libraryTable_new = libraryTable_subset\n",
    "#sgInfoTable_new = sgInfoTable\n",
    "\n",
    "#Select excluded gene fron training set\n",
    "libraryTable_new = libraryTable_research\n",
    "libraryTable_new = libraryTable_research[ libraryTable_research['gene'] == GENE_TO_EXCLUDE ]\n",
    "\n",
    "#libraryTable_subset = libraryTable_research\n",
    "sgInfoTable_new = parseAllSgIds(libraryTable_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "libraryTable_new.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting sgRNA activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate parameters for new sgRNAs\n",
    "#paramTable_new = generateTypicalParamTable(libraryTable_new, sgInfoTable_new, tssTable, p1p2Table, genomeDict, bwhandleDict)\n",
    "paramTable_new = generateTypicalParamTableEx(libraryTable_new, sgInfoTable_new, tssTable, p1p2Table, genomeDict, bwhandleDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pdb\n",
    "#pdb.set_trace()\n",
    "#transform and predict scores according to sgRNA prediction model\n",
    "transformedParams_new = transformParams(paramTable_new, fitTable, estimators)\n",
    "\n",
    "#reconcile any differences in column headers generated by automated binning\n",
    "colTups = []\n",
    "for (l1, l2), col in transformedParams_new.iteritems():\n",
    "    colTups.append((l1,str(l2)))\n",
    "transformedParams_new.columns = pd.MultiIndex.from_tuples(colTups)\n",
    "#iGEM in python 3 .loc with missing column headers is giving issues. So changing it to reindex # can use transformedParams_train if running sequentially otherwise use transformedParamHeader after running above step\n",
    "#predictedScores_new = pd.Series(reg.predict(scaler.transform(transformedParams_new.loc[:, transformedParamHeader.columns].fillna(0).values)), index=transformedParams_new.index)\n",
    "#predictedScores_new = pd.Series(reg.predict(scaler.transform(transformedParams_new.reindex(columns=transformedParamHeader.columns).fillna(0).values)), index=transformedParams_new.index)\n",
    "predictedScores_new = pd.Series(reg.predict(scaler.transform(transformedParams_new.reindex(columns=transformedParams_train.columns).fillna(0).values)), index=transformedParams_new.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedScores_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedScores_new.to_csv(PREDICTED_SCORE_TABLE, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Construct sgRNA libraries\n",
    "## Score sgRNAs for off-target potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are many ways to score sgRNAs as off-target; below is one listed one method that is simple and flexible,\n",
    "#but ignores gapped alignments, alternate PAMs, and uses bowtie which may not be maximally sensitive in all cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iGEM the sequence length can be greather 22. So just added more pluses and adjusteed its length equal to sequence length.\n",
    "#iGEM Revisit to fix phred length properly.\n",
    "\n",
    "#output all sequences to a temporary FASTQ file for running bowtie alignment\n",
    "def outputTempBowtieFastq(libraryTable, outputFileName):\n",
    "    phredString = 'I4!=======44444++++++++++++++++' #weighting for how impactful mismatches are along sgRNA sequence \n",
    "    with open(outputFileName,'w') as outfile:\n",
    "        for name, row in libraryTable.iterrows():\n",
    "            outfile.write('@' + name + '\\n')\n",
    "            outfile.write('CCN' + str(Seq.Seq(row['sequence'][1:]).reverse_complement()) + '\\n')\n",
    "            outfile.write('+\\n')\n",
    "            outfile.write(phredString[0:3+len(str(Seq.Seq(row['sequence'][1:]).reverse_complement()))] + '\\n')\n",
    "            \n",
    "outputTempBowtieFastq(libraryTable_new, TEMP_FASTQ_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "fqFile = TEMP_FASTQ_FILE\n",
    "\n",
    "#specifying a list of parameters to run bowtie with\n",
    "#each tuple contains\n",
    "# *the mismatch threshold below which a site is considered a potential off-target (higher is more stringent)\n",
    "# *the number of sites allowed (1 is minimum since each sgRNA should have one true site in genome)\n",
    "# *the genome index against which to align the sgRNA sequences; these can be custom built to only consider sites near TSSs\n",
    "# *a name for the bowtie run to create appropriately named output files\n",
    "\n",
    "#iGEM\n",
    "#alignmentList = [(39,1,'~/indices/hg19.ensemblTSSflank500b','39_nearTSS'),\n",
    "#                (31,1,'~/indices/hg19.ensemblTSSflank500b','31_nearTSS'),\n",
    "#                (21,1,'~/indices/hg19.maskChrMandPAR','21_genome'),\n",
    "#                (31,2,'~/indices/hg19.ensemblTSSflank500b','31_2_nearTSS'),\n",
    "#                (31,3,'~/indices/hg19.ensemblTSSflank500b','31_3_nearTSS')]\n",
    "\n",
    "#iGEM   No alignments in hg19_maskChrMandPAR that need to be fixed. Need to generate hg19_maskChrMandPAR\n",
    "\n",
    "alignmentList = [(39,1,'../../../../data/large_data_files/indices/hg19.ensemblTSSflank500b','39_nearTSS'),\n",
    "                (31,1,'../../../../data/large_data_files/indices/hg19.ensemblTSSflank500b','31_nearTSS'),\n",
    "                #(21,1,'/data/large_data_files/indices/hg19_maskChrMandPAR','21_genome'),\n",
    "                (21,1,'../../../../data/large_data_files/indices/hg19','21_genome'),\n",
    "                (31,2,'../../../../data/large_data_files/indices/hg19.ensemblTSSflank500b','31_2_nearTSS'),\n",
    "                (31,3,'../../../../data/large_data_files/indices/hg19.ensemblTSSflank500b','31_3_nearTSS')]\n",
    "import os\n",
    "import errno\n",
    "if not os.path.exists('bowtie_output'):\n",
    "    os.makedirs('bowtie_output')\n",
    "\n",
    "alignmentColumns = []\n",
    "for btThreshold, mflag, bowtieIndex, runname in alignmentList:\n",
    "\n",
    "    alignedFile = 'bowtie_output/' + runname + '_aligned.txt'\n",
    "    unalignedFile = 'bowtie_output/' + runname + '_unaligned.fq'\n",
    "    maxFile = 'bowtie_output/' + runname + '_max.fq'\n",
    "    \n",
    "    bowtieString = 'bowtie -n 3 -l 15 -e '+str(btThreshold)+' -m ' + str(mflag) + ' --nomaqround -a --tryhard -p 16 --chunkmbs 256 ' + bowtieIndex + ' --suppress 5,6,7 --un ' + unalignedFile + ' --max ' + maxFile + ' '+ ' -q '+fqFile+' '+ alignedFile\n",
    "    print (bowtieString)\n",
    "    print (subprocess.call(bowtieString, shell=True))\n",
    "\n",
    "    #parse through the file of sgRNAs that exceeded \"m\", the maximum allowable alignments, and mark \"True\" any that are found\n",
    "    \n",
    "    sgsAligning = set()\n",
    "try:    \n",
    "    with open(maxFile) as infile:\n",
    "        sgsAligning = set()\n",
    "        for i, line in enumerate(infile):\n",
    "            if i%4 == 0: #id line\n",
    "                sgsAligning.add(line.strip()[1:])\n",
    "except OSError as e:\n",
    "    if e.errno != errno.EEXIST:\n",
    "        raise\n",
    "\n",
    "    alignmentColumns.append(libraryTable_new.apply(lambda row: row.name in sgsAligning, axis=1))\n",
    "#iGEM zip is an object in python3    \n",
    "#collate results into a table, and flip the boolean values to yield the sgRNAs that passed filter as True\n",
    "alignmentTable = pd.concat(alignmentColumns,axis=1, keys=list(zip(*alignmentList))[3]).ne(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick the top sgRNAs for a library, given predicted activity scores and off-target filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all generated data into one master table\n",
    "predictedScores_new.name = 'predicted score'\n",
    "v2Table = pd.concat((libraryTable_new, predictedScores_new, alignmentTable, sgInfoTable_new), axis=1, keys=['library table v2', 'predicted score', 'off-target filters', 'sgRNA info'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#for our pCRISPRi/a-v2 vector, we append flanking sequences to each sgRNA sequence for cloning and require the oligo to contain\n",
    "#exactly 1 BstXI and BlpI site each for cloning, and exactly 0 SbfI sites for sequencing sample preparation\n",
    "restrictionSites = {re.compile('CCA......TGG'):1,\n",
    "                   re.compile('GCT.AGC'):1,\n",
    "                   re.compile('CCTGCAGG'):0}\n",
    "\n",
    "def matchREsites(sequence, REdict):\n",
    "    seq = sequence.upper()\n",
    "#iGEM in python 3 dict.iteritems is not present. So replace with dict.items        \n",
    "#    for resite, numMatchesExpected in restrictionSites.iteritems():\n",
    "    for resite, numMatchesExpected in restrictionSites.items():\n",
    "        if len(resite.findall(seq)) != numMatchesExpected:\n",
    "            return False\n",
    "        \n",
    "    return True\n",
    "\n",
    "def checkOverlaps(leftPosition, acceptedLeftPositions, nonoverlapMin):\n",
    "    for pos in acceptedLeftPositions:\n",
    "        if abs(pos - leftPosition) < nonoverlapMin:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flanking sequences\n",
    "upstreamConstant = 'CCACCTTGTTG'\n",
    "downstreamConstant = 'GTTTAAGAGCTAAGCTG'\n",
    "\n",
    "#minimum overlap between two sgRNAs targeting the same TSS\n",
    "nonoverlapMin = 3\n",
    "\n",
    "#number of sgRNAs to pick per gene/TSS\n",
    "sgRNAsToPick = 10\n",
    "\n",
    "#iGEM TODO need to enable 21_genome as well. temporarily disabled\n",
    "\n",
    "#list of off-target filter (or combinations of filters) levels, matching the names in the alignment table above\n",
    "offTargetLevels = [['31_nearTSS', '21_genome'],\n",
    "                  ['31_nearTSS'],\n",
    "                  ['21_genome'],\n",
    "                  ['31_2_nearTSS'],\n",
    "                  ['31_3_nearTSS']]\n",
    "\n",
    "#offTargetLevels = [ ['31_nearTSS'],\n",
    "#                  ['31_2_nearTSS'],\n",
    "#                  ['31_3_nearTSS']]\n",
    "\n",
    "\n",
    "#for each gene/TSS, go through each sgRNA in descending order of predicted score\n",
    "#if an sgRNA passes the restriction site, overlap, and off-target filters, accept it into the library\n",
    "#if the number of sgRNAs accepted is less than sgRNAsToPick, reduce off-target stringency by one and continue\n",
    "v2Groups = v2Table.groupby([('library table v2','gene'),('library table v2','transcripts')])\n",
    "newSgIds = []\n",
    "unfinishedTss = []\n",
    "for (gene, transcript), group in v2Groups:\n",
    "    geneSgIds = []\n",
    "    geneLeftPositions = []\n",
    "    empiricalSgIds = dict()\n",
    "    \n",
    "    stringency = 0\n",
    "#iGEM use sort_values instead of sort    \n",
    "    while len(geneSgIds) < sgRNAsToPick and stringency < len(offTargetLevels):\n",
    "        for sgId_v2, row in group.sort_values(('predicted score','predicted score'), ascending=False).iterrows():\n",
    "            oligoSeq = upstreamConstant + row[('library table v2','sequence')] + downstreamConstant\n",
    "            leftPos = row[('sgRNA info', 'position')] - (23 if row[('sgRNA info', 'strand')] == '-' else 0)\n",
    "            if len(geneSgIds) < sgRNAsToPick and row['off-target filters'].loc[offTargetLevels[stringency]].all() \\\n",
    "                and matchREsites(oligoSeq, restrictionSites) \\\n",
    "                and checkOverlaps(leftPos, geneLeftPositions, nonoverlapMin):\n",
    "                geneSgIds.append((sgId_v2,\n",
    "                                  gene,transcript,\n",
    "                                  row[('library table v2','sequence')], oligoSeq,\n",
    "                                  row[('predicted score','predicted score')], np.nan,\n",
    "                                 stringency))\n",
    "                geneLeftPositions.append(leftPos)\n",
    "                \n",
    "        stringency += 1\n",
    "            \n",
    "    if len(geneSgIds) < sgRNAsToPick:\n",
    "        unfinishedTss.append((gene, transcript)) #if the number of accepted sgRNAs is still less than sgRNAsToPick, discard gene\n",
    "    else:\n",
    "        newSgIds.extend(geneSgIds)\n",
    "        \n",
    "libraryTable_complete = pd.DataFrame(newSgIds, columns = ['sgID', 'gene', 'transcript','protospacer sequence', 'oligo sequence',\n",
    " 'predicted score', 'empirical score', 'off-target stringency']).set_index('sgID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of sgRNAs accepted at each stringency level\n",
    "#iGEM newLibraryTable is not defined\n",
    "#newLibraryTable.groupby('off-target stringency').agg(len).iloc[:,0]\n",
    "libraryTable_complete.groupby('off-target stringency').agg(len).iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of TSSs with fewer than required number of sgRNAs (and thus not included in the library)\n",
    "print (len(unfinishedTss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iGEM TODO Do not run this and beyond this cell. It need to be fixed after figuring out missing keys and semantics.\n",
    "\n",
    "#Note that empirical information from previous screens can be included as well--for example:\n",
    "geneToDisc = maxDiscriminantTable['best score'].groupby(level=0).agg(max).to_dict()\n",
    "thresh = 7\n",
    "empiricalBonus = .2\n",
    "\n",
    "upstreamConstant = 'CCACCTTGTTG'\n",
    "downstreamConstant = 'GTTTAAGAGCTAAGCTG'\n",
    "\n",
    "nonoverlapMin = 3\n",
    "\n",
    "sgRNAsToPick = 10\n",
    "\n",
    "offTargetLevels = [['31_nearTSS', '21_genome'],\n",
    "                  ['31_nearTSS'],\n",
    "                  ['21_genome'],\n",
    "                  ['31_2_nearTSS'],\n",
    "                  ['31_3_nearTSS']]\n",
    "offTargetLevels_v1 = [[s + '_v1' for s in l] for l in offTargetLevels]\n",
    "#iGEM comment vGroups and v1Table to make other code work. Need to figure out how tow bring these may be from v1 version \n",
    "#v1Groups = v1Table.groupby([('relative position','gene'),('relative position','transcript')])\n",
    "v2Groups = v2Table.groupby([('library table v2','gene'),('library table v2','transcripts')])\n",
    "\n",
    "newSgIds = []\n",
    "unfinishedTss = []\n",
    "for (gene, transcript), group in v2Groups:\n",
    "    geneSgIds = []\n",
    "    geneLeftPositions = []\n",
    "    empiricalSgIds = dict()\n",
    "    \n",
    "    stringency = 0\n",
    "    \n",
    "    while len(geneSgIds) < sgRNAsToPick and stringency < len(offTargetLevels):\n",
    "        \n",
    "        if gene in geneToDisc and geneToDisc[gene] >= thresh and (gene, transcript) in v1Groups.groups:\n",
    "\n",
    "            for sgId_v1, row in v1Groups.get_group((gene, transcript)).sort(('Empirical activity score','Empirical activity score'),ascending=False).iterrows():\n",
    "                oligoSeq = upstreamConstant + row[('library table v2','sequence')] + downstreamConstant\n",
    "                leftPos = row[('sgRNA info', 'position')] - (23 if row[('sgRNA info', 'strand')] == '-' else 0)\n",
    "                if len(geneSgIds) < sgRNAsToPick and min(abs(row.loc['relative position'].iloc[2:])) < 5000 \\\n",
    "                and row[('Empirical activity score','Empirical activity score')] >= .75 \\\n",
    "                and row['off-target filters'].loc[offTargetLevels_v1[stringency]].all() \\\n",
    "                and matchREsites(oligoSeq, restrictionSites) \\\n",
    "                and checkOverlaps(leftPos, geneLeftPositions, nonoverlapMin):\n",
    "                    if len(geneSgIds) < 2:\n",
    "                        geneSgIds.append((row[('library table v2','sgId_v2')],\n",
    "                                          gene,transcript,\n",
    "                                          row[('library table v2','sequence')], oligoSeq,\n",
    "                                          np.nan,row[('Empirical activity score','Empirical activity score')],\n",
    "                                         stringency))\n",
    "                        geneLeftPositions.append(leftPos)\n",
    "\n",
    "                    empiricalSgIds[row[('library table v2','sgId_v2')]] = row[('Empirical activity score','Empirical activity score')]\n",
    "\n",
    "        adjustedScores = group.apply(lambda row: row[('predicted score','CRISPRiv2 predicted score')] + empiricalBonus if row.name in empiricalSgIds else row[('predicted score','CRISPRiv2 predicted score')], axis=1)\n",
    "        adjustedScores.name = ('adjusted score','')\n",
    "        for sgId_v2, row in pd.concat((group,adjustedScores),axis=1).sort(('adjusted score',''), ascending=False).iterrows():\n",
    "            oligoSeq = upstreamConstant + row[('library table v2','sequence')] + downstreamConstant\n",
    "            leftPos = row[('sgRNA info', 'position')] - (23 if row[('sgRNA info', 'strand')] == '-' else 0)\n",
    "            if len(geneSgIds) < sgRNAsToPick and row['off-target filters'].loc[offTargetLevels[stringency]].all() \\\n",
    "                and matchREsites(oligoSeq, restrictionSites) \\\n",
    "                and checkOverlaps(leftPos, geneLeftPositions, nonoverlapMin):\n",
    "                geneSgIds.append((sgId_v2,\n",
    "                                  gene,transcript,\n",
    "                                  row[('library table v2','sequence')], oligoSeq,\n",
    "                                  row[('predicted score','CRISPRiv2 predicted score')], empiricalSgIds[sgId_v2] if sgId_v2 in empiricalSgIds else np.nan,\n",
    "                                 stringency))\n",
    "                geneLeftPositions.append(leftPos)\n",
    "                \n",
    "        stringency += 1\n",
    "            \n",
    "    if len(geneSgIds) < sgRNAsToPick:\n",
    "        unfinishedTss.append((gene, transcript))\n",
    "    else:\n",
    "        newSgIds.extend(geneSgIds)\n",
    "\n",
    "    \n",
    "libraryTable_complete = pd.DataFrame(newSgIds, columns = ['sgID', 'gene', 'transcript','protospacer sequence', 'oligo sequence',\n",
    " 'predicted score', 'empirical score', 'off-target stringency']).set_index('sgID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design negative controls matching the base composition of the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calcluate the base frequency at each position of the sgRNA, then generate random sequences weighted by this frequency\n",
    "def getBaseFrequencies(libraryTable, baseConversion = {'G':0, 'C':1, 'T':2, 'A':3}):\n",
    "    baseArray = np.zeros((len(libraryTable),20))\n",
    "\n",
    "    for i, (index, seq) in enumerate(libraryTable['protospacer sequence'].iteritems()):\n",
    "        for j, char in enumerate(seq.upper()):\n",
    "            baseArray[i,j] = baseConversion[char]\n",
    "\n",
    "    baseTable = pd.DataFrame(baseArray, index = libraryTable.index)\n",
    "    \n",
    "    baseFrequencies = baseTable.apply(lambda col: col.groupby(col).agg(len)).fillna(0) / len(baseTable)\n",
    "    baseFrequencies.index = ['G','C','T','A']\n",
    "    \n",
    "    baseCumulativeFrequencies = baseFrequencies.copy()\n",
    "    baseCumulativeFrequencies.loc['C'] = baseFrequencies.loc['G'] + baseFrequencies.loc['C']\n",
    "    baseCumulativeFrequencies.loc['T'] = baseFrequencies.loc['G'] + baseFrequencies.loc['C'] + baseFrequencies.loc['T']\n",
    "    baseCumulativeFrequencies.loc['A'] = baseFrequencies.loc['G'] + baseFrequencies.loc['C'] + baseFrequencies.loc['T'] + baseFrequencies.loc['A']\n",
    "\n",
    "    return baseFrequencies, baseCumulativeFrequencies\n",
    "\n",
    "def generateRandomSequence(baseCumulativeFrequencies):\n",
    "    randArray = np.random.random(baseCumulativeFrequencies.shape[1])\n",
    "    \n",
    "    seq = []\n",
    "    for i, col in baseCumulativeFrequencies.iteritems():\n",
    "        for base, freq in col.iteritems():\n",
    "            if randArray[i] < freq:\n",
    "                seq.append(base)\n",
    "                break\n",
    "                \n",
    "    return ''.join(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pdb\n",
    "#pdb.set_trace()\n",
    "\n",
    "baseCumulativeFrequencies = getBaseFrequencies(libraryTable_complete)[1]\n",
    "negList = []\n",
    "#for i in range(30000):\n",
    "#    negList.append(generateRandomSequence(baseCumulativeFrequencies))\n",
    "#negTable = pd.DataFrame(negList, index=['non-targeting_' + str(i) for i in range(30000)], columns = ['sequence'])\n",
    "\n",
    "numberToGenerate = 10 #can generate many more; some will be filtered out by off-targets, and you can always select an arbitrary subset for inclusion into the library\n",
    "for i in range(numberToGenerate):\n",
    "    negList.append(generateRandomSequence(baseCumulativeFrequencies))\n",
    "negTable = pd.DataFrame(negList, index=['non-targeting_' + str(i) for i in range(numberToGenerate)], columns = ['sequence'])\n",
    "\n",
    "\n",
    "outputTempBowtieFastq(negTable, TEMP_FASTQ_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#similar to targeting sgRNA off-target scoring, but looking for sgRNAs with 0 alignments\n",
    "fqFile = TEMP_FASTQ_FILE\n",
    "\n",
    "alignmentList = [(31,1,'~/indices/hg19.ensemblTSSflank500b','31_nearTSS_negs'),\n",
    "                (21,1,'~/indices/hg19.maskChrMandPAR','21_genome_negs')]\n",
    "\n",
    "alignmentColumns = []\n",
    "for btThreshold, mflag, bowtieIndex, runname in alignmentList:\n",
    "\n",
    "    alignedFile = 'bowtie_output/' + runname + '_aligned.txt'\n",
    "    unalignedFile = 'bowtie_output//' + runname + '_unaligned.fq'\n",
    "    maxFile = 'bowtie_output/' + runname + '_max.fq'\n",
    "    \n",
    "    bowtieString = 'bowtie -n 3 -l 15 -e '+str(btThreshold)+' -m ' + str(mflag) + ' --nomaqround -a --tryhard -p 16 --chunkmbs 256 ' + bowtieIndex + ' --suppress 5,6,7 --un ' + unalignedFile + ' --max ' + maxFile + ' '+ ' -q '+fqFile+' '+ alignedFile\n",
    "    print bowtieString\n",
    "    print subprocess.call(bowtieString, shell=True)\n",
    "\n",
    "    #read unaligned file for negs, and then don't flip boolean of alignmentTable\n",
    "    with open(unalignedFile) as infile:\n",
    "        sgsAligning = set()\n",
    "        for i, line in enumerate(infile):\n",
    "            if i%4 == 0: #id line\n",
    "                sgsAligning.add(line.strip()[1:])\n",
    "\n",
    "    alignmentColumns.append(negTable.apply(lambda row: row.name in sgsAligning, axis=1))\n",
    "    \n",
    "alignmentTable = pd.concat(alignmentColumns,axis=1, keys=zip(*alignmentList)[3])\n",
    "alignmentTable.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acceptedNegList = []\n",
    "negCount = 0\n",
    "for i, (name, row) in enumerate(pd.concat((negTable,alignmentTable),axis=1, keys=['seq','alignment']).iterrows()):\n",
    "    oligo = upstreamConstant + row['seq','sequence'] + downstreamConstant\n",
    "    if row['alignment'].all() and matchREsites(oligo, restrictionSites):\n",
    "        acceptedNegList.append(('non-targeting_%05d' % negCount, 'negative_control', 'na', row['seq','sequence'], oligo, 0))\n",
    "        negCount += 1\n",
    "        \n",
    "acceptedNegs = pd.DataFrame(acceptedNegList, columns = ['sgId', 'gene', 'transcript', 'protospacer sequence', 'oligo sequence', 'off-target stringency']).set_index('sgId')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalizing library design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* divide genes into sublibrary groups (if required)\n",
    "* assign negative control sgRNAs to sublibrary groups; ~1-2% of the number of sgRNAs in the library is a good rule-of-thumb\n",
    "* append PCR adapter sequences (~18bp) to each end of the oligo sequences to enable amplification of the oligo pool; each sublibary should have an orthogonal sequence so they can be cloned separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#import pdb\n",
    "#pdb.set_trace()\n",
    "print (sys.version)\n",
    "#transformedParamHeader.head()\n",
    "#transformedParamHeader=transformedParamsTrainHead\n",
    "#paramTable_trainingGuides.describe()\n",
    "from sklearn import linear_model\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])\n",
    "#LinearRegression()\n",
    "reg.coef_\n",
    "reg.intercept_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
